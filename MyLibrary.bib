
@article{akhtar_threat_2018,
	title = {Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey},
	volume = {6},
	issn = {2169-3536},
	doi = {10.1109/ACCESS.2018.2807385},
	shorttitle = {Threat of Adversarial Attacks on Deep Learning in Computer Vision},
	abstract = {Deep learning is at the heart of the current rise of artificial intelligence. In the field of computer vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. Whereas, deep neural networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the deep learning models. Adversarial attacks pose a serious threat to the success of deep learning in practice. This fact has recently led to a large influx of contributions in this direction. This paper presents the first comprehensive survey on adversarial attacks on deep learning in computer vision. We review the works that design adversarial attacks, analyze the existence of such attacks and propose defenses against them. To emphasize that adversarial attacks are possible in practical conditions, we separately review the contributions that evaluate adversarial attacks in the real-world scenarios. Finally, drawing on the reviewed literature, we provide a broader outlook of this research direction.},
	pages = {14410--14430},
	journaltitle = {{IEEE} Access},
	author = {Akhtar, N. and Mian, A.},
	date = {2018},
	keywords = {adversarial attacks, adversarial learning, adversarial perturbation, artificial intelligence, black-box attack, Computational modeling, computer vision, Computer vision, Deep learning, deep learning models, deep neural networks, humanities, learning (artificial intelligence), Machine learning, neural nets, Neural networks, perturbation detection, Perturbation methods, Predictive models, self-driving cars, Task analysis, white-box attack},
	file = {IEEE Xplore Abstract Record:/Users/tiffanychien/Zotero/storage/GW453795/8294186.html:text/html;IEEE Xplore Full Text PDF:/Users/tiffanychien/Zotero/storage/XFMPXRRF/Akhtar and Mian - 2018 - Threat of Adversarial Attacks on Deep Learning in .pdf:application/pdf}
}

@article{alzantot_generating_2018,
	title = {Generating Natural Language Adversarial Examples},
	url = {http://arxiv.org/abs/1804.07998},
	abstract = {Deep neural networks ({DNNs}) are vulnerable to adversarial examples, perturbations to correctly classified examples which can cause the model to misclassify. In the image domain, these perturbations are often virtually indistinguishable to human perception, causing humans and state-of-the-art models to disagree. However, in the natural language domain, small perturbations are clearly perceptible, and the replacement of a single word can drastically alter the semantics of the document. Given these challenges, we use a black-box population-based optimization algorithm to generate semantically and syntactically similar adversarial examples that fool well-trained sentiment analysis and textual entailment models with success rates of 97\% and 70\%, respectively. We additionally demonstrate that 92.3\% of the successful sentiment analysis adversarial examples are classified to their original label by 20 human annotators, and that the examples are perceptibly quite similar. Finally, we discuss an attempt to use adversarial training as a defense, but fail to yield improvement, demonstrating the strength and diversity of our adversarial examples. We hope our findings encourage researchers to pursue improving the robustness of {DNNs} in the natural language domain.},
	journaltitle = {{arXiv}:1804.07998 [cs]},
	author = {Alzantot, Moustafa and Sharma, Yash and Elgohary, Ahmed and Ho, Bo-Jhang and Srivastava, Mani and Chang, Kai-Wei},
	urldate = {2019-06-05},
	date = {2018-04-21},
	eprinttype = {arxiv},
	eprint = {1804.07998},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1804.07998 PDF:/Users/tiffanychien/Zotero/storage/7L9Z3UUN/Alzantot et al. - 2018 - Generating Natural Language Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/JXEQKYUY/1804.html:text/html}
}

@article{serban_adversarial_2018,
	title = {Adversarial Examples - A Complete Characterisation of the Phenomenon},
	url = {http://arxiv.org/abs/1810.01185},
	abstract = {We provide a complete characterisation of the phenomenon of adversarial examples - inputs intentionally crafted to fool machine learning models. We aim to cover all the important concerns in this field of study: (1) the conjectures on the existence of adversarial examples, (2) the security, safety and robustness implications, (3) the methods used to generate and (4) protect against adversarial examples and (5) the ability of adversarial examples to transfer between different machine learning models. We provide ample background information in an effort to make this document self-contained. Therefore, this document can be used as survey, tutorial or as a catalog of attacks and defences using adversarial examples.},
	journaltitle = {{arXiv}:1810.01185 [cs]},
	author = {Serban, Alexandru Constantin and Poll, Erik and Visser, Joost},
	urldate = {2019-06-05},
	date = {2018-10-02},
	eprinttype = {arxiv},
	eprint = {1810.01185},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1810.01185 PDF:/Users/tiffanychien/Zotero/storage/583D94MJ/Serban et al. - 2018 - Adversarial Examples - A Complete Characterisation.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/9M4NT4XC/1810.html:text/html}
}

@article{hu_parabank:_2019,
	title = {{ParaBank}: Monolingual Bitext Generation and Sentential Paraphrasing via Lexically-constrained Neural Machine Translation},
	url = {http://arxiv.org/abs/1901.03644},
	shorttitle = {{ParaBank}},
	abstract = {We present {ParaBank}, a large-scale English paraphrase dataset that surpasses prior work in both quantity and quality. Following the approach of {ParaNMT}, we train a Czech-English neural machine translation ({NMT}) system to generate novel paraphrases of English reference sentences. By adding lexical constraints to the {NMT} decoding procedure, however, we are able to produce multiple high-quality sentential paraphrases per source sentence, yielding an English paraphrase resource with more than 4 billion generated tokens and exhibiting greater lexical diversity. Using human judgments, we also demonstrate that {ParaBank}'s paraphrases improve over {ParaNMT} on both semantic similarity and fluency. Finally, we use {ParaBank} to train a monolingual {NMT} model with the same support for lexically-constrained decoding for sentence rewriting tasks.},
	journaltitle = {{arXiv}:1901.03644 [cs]},
	author = {Hu, J. Edward and Rudinger, Rachel and Post, Matt and Van Durme, Benjamin},
	urldate = {2019-06-05},
	date = {2019-01-11},
	eprinttype = {arxiv},
	eprint = {1901.03644},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1901.03644 PDF:/Users/tiffanychien/Zotero/storage/JVEW66FP/Hu et al. - 2019 - ParaBank Monolingual Bitext Generation and Senten.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/MARR5CFZ/1901.html:text/html}
}

@article{alshemali_text_nodate,
	title = {(text) Adversarial Attacks to Deep Neural Networks in {NLP}: A Survey},
	abstract = {Deep learning models have achieved great success in solving a variety of natural language processing ({NLP}) problems. However, an ever-growing body of research illustrates their vulnerability to adversarial examples – inputs modiﬁed by introducing small perturbations to deliberately fool a target model into outputting incorrect results. This vulnerability to adversarial examples has become one of the main hurdles in applying deep neural networks in safety-critical environments. This paper presents a comprehensive survey of adversarial attacks in the ﬁeld of {NLP}. In this survey, we consider contemporary research on the usage of adversarial examples to foil {DNNs} and present an exhaustive overview of the methods for creating adversarial texts.},
	pages = {83},
	author = {Alshemali, Basemah and Kalita, Jugal},
	langid = {english},
	file = {nlp_Whole_Survey.pdf:/Users/tiffanychien/Documents/colorado/nlp_Whole_Survey.pdf:application/pdf}
}

@inproceedings{jia_adversarial_2017,
	location = {Copenhagen, Denmark},
	title = {Adversarial Examples for Evaluating Reading Comprehension Systems},
	url = {http://aclweb.org/anthology/D17-1215},
	doi = {10.18653/v1/D17-1215},
	abstract = {Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset ({SQuAD}). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of 75\% F1 score to 36\%; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to 7\%. We hope our insights will motivate the development of new models that understand language more precisely.},
	eventtitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural           Language Processing},
	pages = {2021--2031},
	booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural           Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Jia, Robin and Liang, Percy},
	urldate = {2019-06-05},
	date = {2017},
	langid = {english},
	file = {jia liang 13 adding sentences.pdf:/Users/tiffanychien/Documents/colorado/jia liang 13 adding sentences.pdf:application/pdf}
}

@article{ribeiro_semantically_2018,
	title = {Semantically Equivalent Adversarial Rules for Debugging {NLP} models},
	abstract = {Complex machine learning models for {NLP} are often brittle, making different predictions for input instances that are extremely similar semantically. To automatically detect this behavior for individual instances, we present semantically equivalent adversaries ({SEAs}) – semantic-preserving perturbations that induce changes in the model’s predictions. We generalize these adversaries into semantically equivalent adversarial rules ({SEARs}) – simple, universal replacement rules that induce adversaries on many instances. We demonstrate the usefulness and ﬂexibility of {SEAs} and {SEARs} by detecting bugs in black-box state-of-the-art models for three domains: machine comprehension, visual questionanswering, and sentiment analysis. Via user studies, we demonstrate that we generate high-quality local adversaries for more instances than humans, and that {SEARs} induce four times as many mistakes as the bugs discovered by human experts. {SEARs} are also actionable: retraining models using data augmentation signiﬁcantly reduces bugs, while maintaining accuracy.},
	pages = {10},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	date = {2018},
	langid = {english},
	file = {ribeiro 53 paraphrase.pdf:/Users/tiffanychien/Documents/colorado/ribeiro 53 paraphrase.pdf:application/pdf}
}

@inproceedings{iyyer_adversarial_2018,
	location = {New Orleans, Louisiana},
	title = {Adversarial Example Generation with Syntactically Controlled Paraphrase Networks},
	url = {https://www.aclweb.org/anthology/N18-1170},
	doi = {10.18653/v1/N18-1170},
	abstract = {We propose syntactically controlled paraphrase networks ({SCPNs}) and use them to generate adversarial examples. Given a sentence and a target syntactic form (e.g., a constituency parse), {SCPNs} are trained to produce a paraphrase of the sentence with the desired syntax. We show it is possible to create training data for this task by first doing backtranslation at a very large scale, and then using a parser to label the syntactic transformations that naturally occur during this process. Such data allows us to train a neural encoder-decoder model with extra inputs to specify the target syntax. A combination of automated and human evaluations show that {SCPNs} generate paraphrases that follow their target specifications without decreasing paraphrase quality when compared to baseline (uncontrolled) paraphrase systems. Furthermore, they are more capable of generating syntactically adversarial examples that both (1) “fool” pretrained models and (2) improve the robustness of these models to syntactic variation when used to augment their training data.},
	pages = {1875--1885},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Iyyer, Mohit and Wieting, John and Gimpel, Kevin and Zettlemoyer, Luke},
	urldate = {2019-06-05},
	date = {2018-06},
	file = {Full Text PDF:/Users/tiffanychien/Zotero/storage/2N6QSSLE/Iyyer et al. - 2018 - Adversarial Example Generation with Syntactically .pdf:application/pdf}
}

@inproceedings{khandelwal_sharp_2018,
	location = {Melbourne, Australia},
	title = {Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context},
	url = {https://www.aclweb.org/anthology/P18-1027},
	shorttitle = {Sharp Nearby, Fuzzy Far Away},
	abstract = {We know very little about how neural language models ({LM}) use prior linguistic context. In this paper, we investigate the role of context in an {LSTM} {LM}, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and {WikiText}-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the {LSTM} to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural {LMs} use their context, but also sheds light on recent success from cache-based models.},
	pages = {284--294},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
	urldate = {2019-06-05},
	date = {2018-07},
	file = {Full Text PDF:/Users/tiffanychien/Zotero/storage/DDNUVEXE/Khandelwal et al. - 2018 - Sharp Nearby, Fuzzy Far Away How Neural Language .pdf:application/pdf}
}

@article{al-dubaee_new_2010,
	title = {New Strategy of Lossy Text Compression},
	volume = {0},
	issn = {978-0-7695-4152-5},
	doi = {10.1109/ICIIC.2010.51},
	abstract = {This paper proposes a new strategy that is based on the signal processing tools applied to text compression of files namely, the wavelet transform and the fourier transform. The influence of compression size and threshold of wavelet filters and the fourier transform as well as two parameters: families of wavelet filters and decomposition levels, on compression factor of text files are investigated. The experimental results are shown that the wavelet and the fourier transforms are suitable for lossy text compression with non-stationary text signal files. In addition, the fourier transform is the most suitable with files which have same characters such as aaa.txt and aaaa.txt files. However, the results of wavelet and fourier transforms are loss less text compression with stationary text signal files (aaa.txt and aaaa.txt files). This research also represents a step forwards dealing with both images and text compression i.e. multimedia compression.},
	pages = {22--26},
	journaltitle = {Integrated Intelligent Computing},
	shortjournal = {Integrated Intelligent Computing},
	author = {Al-Dubaee, Shawki and Ahmad, Nesar},
	date = {2010-08-05},
	file = {Full Text PDF:/Users/tiffanychien/Zotero/storage/9ZUTM7WU/Al-Dubaee and Ahmad - 2010 - New Strategy of Lossy Text Compression.pdf:application/pdf}
}

@article{lin_bandlimiting_2019,
	title = {Bandlimiting Neural Networks Against Adversarial Attacks},
	url = {http://arxiv.org/abs/1905.12797},
	abstract = {In this paper, we study the adversarial attack and defence problem in deep learning from the perspective of Fourier analysis. We first explicitly compute the Fourier transform of deep {ReLU} neural networks and show that there exist decaying but non-zero high frequency components in the Fourier spectrum of neural networks. We demonstrate that the vulnerability of neural networks towards adversarial samples can be attributed to these insignificant but non-zero high frequency components. Based on this analysis, we propose to use a simple post-averaging technique to smooth out these high frequency components to improve the robustness of neural networks against adversarial attacks. Experimental results on the {ImageNet} dataset have shown that our proposed method is universally effective to defend many existing adversarial attacking methods proposed in the literature, including {FGSM}, {PGD}, {DeepFool} and C\&W attacks. Our post-averaging method is simple since it does not require any re-training, and meanwhile it can successfully defend over 95\% of the adversarial samples generated by these methods without introducing any significant performance degradation (less than 1\%) on the original clean images.},
	journaltitle = {{arXiv}:1905.12797 [cs, stat]},
	author = {Lin, Yuping and A., Kasra Ahmadi K. and Jiang, Hui},
	urldate = {2019-06-05},
	date = {2019-05-29},
	eprinttype = {arxiv},
	eprint = {1905.12797},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, I.1.5, Statistics - Machine Learning},
	file = {arXiv\:1905.12797 PDF:/Users/tiffanychien/Zotero/storage/ZMVK55LU/Lin et al. - 2019 - Bandlimiting Neural Networks Against Adversarial A.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/U5W5LXD2/1905.html:text/html}
}

@article{goodfellow_explaining_2014,
	title = {Explaining and Harnessing Adversarial Examples},
	url = {http://arxiv.org/abs/1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the {MNIST} dataset.},
	journaltitle = {{arXiv}:1412.6572 [cs, stat]},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	urldate = {2019-06-05},
	date = {2014-12-19},
	eprinttype = {arxiv},
	eprint = {1412.6572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1412.6572 PDF:/Users/tiffanychien/Zotero/storage/GJ5MN7YM/Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/65WXK3FM/1412.html:text/html}
}

@inproceedings{zheng_robust_2018,
	title = {Robust Detection of Adversarial Attacks by Modeling the Intrinsic Properties of Deep Neural Networks},
	abstract = {It has been shown that deep neural network ({DNN}) based classifiers are vulnerable to human-imperceptive adversarial perturbations which can cause {DNN} classifiers to output wrong predictions with high confidence. We propose an unsupervised learning approach to detect adversarial inputs without any knowledge of attackers. Our approach tries to capture the intrinsic properties of a {DNN} classifier and uses them to detect adversarial inputs. The intrinsic properties used in this study are the output distributions of the hidden neurons in a {DNN} classifier presented with natural images. Our approach can be easily applied to any {DNN} classifiers or combined with other defense strategies to improve robustness. Experimental results show that our approach demonstrates state-of-the-art robustness in defending black-box and gray-box attacks.},
	booktitle = {{NeurIPS}},
	author = {Zheng, Zhihao and Hong, Pengyu},
	date = {2018},
	keywords = {Approximation algorithm, Artificial neural network, Black box, Gm(m), Iterative method, Learning classifier system, Neural network software, Unsupervised learning},
	file = {Full Text PDF:/Users/tiffanychien/Zotero/storage/46VIRDX5/Zheng and Hong - 2018 - Robust Detection of Adversarial Attacks by Modelin.pdf:application/pdf}
}

@article{belinkov_synthetic_2017,
	title = {Synthetic and Natural Noise Both Break Neural Machine Translation},
	url = {http://arxiv.org/abs/1711.02173},
	abstract = {Character-based neural machine translation ({NMT}) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront {NMT} models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.},
	journaltitle = {{arXiv}:1711.02173 [cs]},
	author = {Belinkov, Yonatan and Bisk, Yonatan},
	urldate = {2019-06-05},
	date = {2017-11-06},
	eprinttype = {arxiv},
	eprint = {1711.02173},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, I.2.7},
	file = {arXiv\:1711.02173 PDF:/Users/tiffanychien/Zotero/storage/ULQX8VTN/Belinkov and Bisk - 2017 - Synthetic and Natural Noise Both Break Neural Mach.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/MAA2QDAG/1711.html:text/html}
}

@article{mudrakarta_did_2018,
	title = {Did the Model Understand the Question?},
	url = {http://arxiv.org/abs/1805.05492},
	abstract = {We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of {\textbackslash}emph\{attribution\} (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from \$61.1{\textbackslash}\%\$ to \$19{\textbackslash}\%\$, and that of a tabular question answering model from \$33.5{\textbackslash}\%\$ to \$3.3{\textbackslash}\%\$. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.},
	journaltitle = {{arXiv}:1805.05492 [cs]},
	author = {Mudrakarta, Pramod Kaushik and Taly, Ankur and Sundararajan, Mukund and Dhamdhere, Kedar},
	urldate = {2019-06-05},
	date = {2018-05-14},
	eprinttype = {arxiv},
	eprint = {1805.05492},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1805.05492 PDF:/Users/tiffanychien/Zotero/storage/ADHEEYVE/Mudrakarta et al. - 2018 - Did the Model Understand the Question.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/C5KWHZ2H/1805.html:text/html}
}

@article{blohm_comparing_2018,
	title = {Comparing Attention-based Convolutional and Recurrent Neural Networks: Success and Limitations in Machine Reading Comprehension},
	url = {http://arxiv.org/abs/1808.08744},
	shorttitle = {Comparing Attention-based Convolutional and Recurrent Neural Networks},
	abstract = {We propose a machine reading comprehension model based on the compare-aggregate framework with two-staged attention that achieves state-of-the-art results on the {MovieQA} question answering dataset. To investigate the limitations of our model as well as the behavioral difference between convolutional and recurrent neural networks, we generate adversarial examples to confuse the model and compare to human performance. Furthermore, we assess the generalizability of our model by analyzing its differences to human inference,},
	journaltitle = {{arXiv}:1808.08744 [cs]},
	author = {Blohm, Matthias and Jagfeld, Glorianna and Sood, Ekta and Yu, Xiang and Vu, Ngoc Thang},
	urldate = {2019-06-05},
	date = {2018-08-27},
	eprinttype = {arxiv},
	eprint = {1808.08744},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1808.08744 PDF:/Users/tiffanychien/Zotero/storage/3WXNHGSJ/Blohm et al. - 2018 - Comparing Attention-based Convolutional and Recurr.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/MSFG7MHY/1808.html:text/html}
}

@article{li_visualizing_2015,
	title = {Visualizing and Understanding Neural Models in {NLP}},
	url = {http://arxiv.org/abs/1506.01066},
	abstract = {While neural networks have been successfully applied to many {NLP} tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve \{{\textbackslash}em compositionality\}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for {NLP}, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's \{{\textbackslash}em salience\}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) {LSTM}-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and {LSTMs}. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why {LSTMs} outperform simple recurrent nets,},
	journaltitle = {{arXiv}:1506.01066 [cs]},
	author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
	urldate = {2019-06-05},
	date = {2015-06-02},
	eprinttype = {arxiv},
	eprint = {1506.01066},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1506.01066 PDF:/Users/tiffanychien/Zotero/storage/E4AVS37U/Li et al. - 2015 - Visualizing and Understanding Neural Models in NLP.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/B97WQVK6/1506.html:text/html}
}

@article{bowman_generating_2015,
	title = {Generating Sentences from a Continuous Space},
	url = {http://arxiv.org/abs/1511.06349},
	abstract = {The standard recurrent neural network language model ({RNNLM}) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an {RNN}-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
	journaltitle = {{arXiv}:1511.06349 [cs]},
	author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
	urldate = {2019-06-05},
	date = {2015-11-19},
	eprinttype = {arxiv},
	eprint = {1511.06349},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1511.06349 PDF:/Users/tiffanychien/Zotero/storage/EVIPJG5L/Bowman et al. - 2015 - Generating Sentences from a Continuous Space.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/GCW5YYHF/1511.html:text/html}
}

@article{gil_white--black:_2019,
	title = {White-to-Black: Efficient Distillation of Black-Box Adversarial Attacks},
	url = {http://arxiv.org/abs/1904.02405},
	shorttitle = {White-to-Black},
	abstract = {Adversarial examples are important for understanding the behavior of neural models, and can improve their robustness through adversarial training. Recent work in natural language processing generated adversarial examples by assuming white-box access to the attacked model, and optimizing the input directly against it (Ebrahimi et al., 2018). In this work, we show that the knowledge implicit in the optimization procedure can be distilled into another more efficient neural network. We train a model to emulate the behavior of a white-box attack and show that it generalizes well across examples. Moreover, it reduces adversarial example generation time by 19x-39x. We also show that our approach transfers to a black-box setting, by attacking The Google Perspective {API} and exposing its vulnerability. Our attack flips the {API}-predicted label in 42{\textbackslash}\% of the generated examples, while humans maintain high-accuracy in predicting the gold label.},
	journaltitle = {{arXiv}:1904.02405 [cs, stat]},
	author = {Gil, Yotam and Chai, Yoav and Gorodissky, Or and Berant, Jonathan},
	urldate = {2019-06-05},
	date = {2019-04-04},
	eprinttype = {arxiv},
	eprint = {1904.02405},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1904.02405 PDF:/Users/tiffanychien/Zotero/storage/NB7FHNKG/Gil et al. - 2019 - White-to-Black Efficient Distillation of Black-Bo.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/FWYCMSG8/1904.html:text/html}
}

@inproceedings{papernot_distillation_2016,
	title = {Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},
	doi = {10.1109/SP.2016.41},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network ({DNN}) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the {DNN}, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on {DNNs}. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training {DNNs}. We also empirically study the effectiveness of our defense mechanisms on two {DNNs} placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied {DNN}. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800\% on one of the {DNNs} we tested.},
	eventtitle = {2016 {IEEE} Symposium on Security and Privacy ({SP})},
	pages = {582--597},
	booktitle = {2016 {IEEE} Symposium on Security and Privacy ({SP})},
	author = {Papernot, N. and {McDaniel}, P. and Wu, X. and Jha, S. and Swami, A.},
	date = {2016-05},
	keywords = {adversarial perturbations, adversarial sample creation, Automobiles, Computational modeling, Computer architecture, deep learning algorithms, deep neural networks, defensive distillation mechanism, {DNN}, learning (artificial intelligence), Machine learning, machine learning problems, neural nets, Neural networks, security, Security, security of data, Training},
	file = {IEEE Xplore Abstract Record:/Users/tiffanychien/Zotero/storage/4K5DHCWG/7546524.html:text/html;IEEE Xplore Full Text PDF:/Users/tiffanychien/Zotero/storage/79YW2L4M/Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}

@article{cifka_are_2018,
	title = {Are {BLEU} and Meaning Representation in Opposition?},
	url = {http://arxiv.org/abs/1805.06536},
	abstract = {One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation ({NMT}) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive {NMT} architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.},
	journaltitle = {{arXiv}:1805.06536 [cs]},
	author = {Cífka, Ondřej and Bojar, Ondřej},
	urldate = {2019-06-06},
	date = {2018-05-16},
	eprinttype = {arxiv},
	eprint = {1805.06536},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1805.06536 PDF:/Users/tiffanychien/Zotero/storage/IRITMCA4/Cífka and Bojar - 2018 - Are BLEU and Meaning Representation in Opposition.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/NJX3ZG72/1805.html:text/html}
}

@article{cifka_are_2018-1,
	title = {Are {BLEU} and Meaning Representation in Opposition?},
	url = {http://arxiv.org/abs/1805.06536},
	abstract = {One of possible ways of obtaining continuous-space sentence representations is by training neural machine translation ({NMT}) systems. The recent attention mechanism however removes the single point in the neural network from which the source sentence representation can be extracted. We propose several variations of the attentive {NMT} architecture bringing this meeting point back. Empirical evaluation suggests that the better the translation quality, the worse the learned sentence representations serve in a wide range of classification and similarity tasks.},
	journaltitle = {{arXiv}:1805.06536 [cs]},
	author = {Cífka, Ondřej and Bojar, Ondřej},
	urldate = {2019-06-06},
	date = {2018-05-16},
	eprinttype = {arxiv},
	eprint = {1805.06536},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1805.06536 PDF:/Users/tiffanychien/Zotero/storage/ZRQGILG2/Cífka and Bojar - 2018 - Are BLEU and Meaning Representation in Opposition.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/ZGE585P9/1805.html:text/html}
}

@article{arora_simple_2016,
	title = {A Simple but Tough-to-Beat Baseline for Sentence Embeddings},
	url = {https://openreview.net/forum?id=SyK00v5xx},
	abstract = {The success of neural network methods for computing word embeddings has motivated methods for generating semantic embeddings of longer pieces of text, such as sentences and paragraphs....},
	author = {Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
	urldate = {2019-06-06},
	date = {2016-11-04},
	file = {Full Text PDF:/Users/tiffanychien/Zotero/storage/68FB3K3J/Arora et al. - 2016 - A Simple but Tough-to-Beat Baseline for Sentence E.pdf:application/pdf;Snapshot:/Users/tiffanychien/Zotero/storage/N8LLLRH7/forum.html:text/html}
}

@inproceedings{templeton_exploring_2018,
	title = {Exploring Sentence Vector Spaces through Automatic Summarization},
	doi = {10.1109/ICMLA.2018.00016},
	abstract = {Given vector representations for individual words, it is necessary to compute vector representations of sentences for many applications in a compositional manner, often using artificial neural networks. Relatively little work has explored the internal structure and properties of such sentence vectors. In this paper, we explore the properties of sentence vectors in the context of automatic summarization. In particular, we show that cosine similarity between sentence vectors and document vectors is strongly correlated with sentence importance and that vector semantics can identify and correct gaps between the sentences chosen so far and the document. In addition, we identify specific dimensions which are linked to effective summaries. To our knowledge, this is the first time specific dimensions of sentence embeddings have been connected to sentence properties. We also compare the features of different methods of sentence embeddings. Many of these insights have applications in uses of sentence embeddings far beyond summarization.},
	eventtitle = {2018 17th {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
	pages = {55--60},
	booktitle = {2018 17th {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
	author = {Templeton, A. and Kalita, J.},
	date = {2018-12},
	keywords = {artificial neural networks, automatic summarization, Computational modeling, document handling, document vectors, Force, natural language processing, Natural Language Processing, neural nets, Neural networks, {NLP}, Redundancy, Semantics, sentence embeddings, sentence importance, sentence properties, sentence vector spaces, sentence vectors, Sentence Vectors, Space exploration, Summarization, Task analysis, vector representations, vector semantics, vectors},
	file = {IEEE Xplore Abstract Record:/Users/tiffanychien/Zotero/storage/2MHYCYRN/8614041.html:text/html;IEEE Xplore Full Text PDF:/Users/tiffanychien/Zotero/storage/WZCACHMF/Templeton and Kalita - 2018 - Exploring Sentence Vector Spaces through Automatic.pdf:application/pdf}
}

@article{cer_universal_2018,
	title = {Universal Sentence Encoder},
	url = {http://arxiv.org/abs/1803.11175},
	abstract = {We present models for encoding sentences into embedding vectors that specifically target transfer learning to other {NLP} tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning. We find that transfer learning using sentence embeddings tends to outperform word level transfer. With transfer learning via sentence embeddings, we observe surprisingly good performance with minimal amounts of supervised training data for a transfer task. We obtain encouraging results on Word Embedding Association Tests ({WEAT}) targeted at detecting model bias. Our pre-trained sentence encoding models are made freely available for download and on {TF} Hub.},
	journaltitle = {{arXiv}:1803.11175 [cs]},
	author = {Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and Sung, Yun-Hsuan and Strope, Brian and Kurzweil, Ray},
	urldate = {2019-06-06},
	date = {2018-03-29},
	eprinttype = {arxiv},
	eprint = {1803.11175},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1803.11175 PDF:/Users/tiffanychien/Zotero/storage/RAUTPW97/Cer et al. - 2018 - Universal Sentence Encoder.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/ZHMYPFXS/1803.html:text/html}
}

@article{conneau_supervised_2017,
	title = {Supervised Learning of Universal Sentence Representations from Natural Language Inference Data},
	url = {http://arxiv.org/abs/1705.02364},
	abstract = {Many modern {NLP} systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like {SkipThought} vectors on a wide range of transfer tasks. Much like how computer vision uses {ImageNet} to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other {NLP} tasks. Our encoder is publicly available.},
	journaltitle = {{arXiv}:1705.02364 [cs]},
	author = {Conneau, Alexis and Kiela, Douwe and Schwenk, Holger and Barrault, Loic and Bordes, Antoine},
	urldate = {2019-06-06},
	date = {2017-05-05},
	eprinttype = {arxiv},
	eprint = {1705.02364},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1705.02364 PDF:/Users/tiffanychien/Zotero/storage/B4UHKUQW/Conneau et al. - 2017 - Supervised Learning of Universal Sentence Represen.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/F8GYTQ3Y/1705.html:text/html}
}

@article{quan_efficient_2019,
	title = {An Efficient Framework for Sentence Similarity Modeling},
	volume = {27},
	issn = {2329-9290},
	doi = {10.1109/TASLP.2019.2899494},
	abstract = {Sentence similarity modeling lies at the core of many natural language processing applications, and thus has received much attention. Owing to the success of word embeddings, recently, popular neural network methods achieved sentence embedding. Most of them focused on learning semantic information and modeling it as a continuous vector, yet the syntactic information of sentences has not been fully exploited. On the other hand, prior works have shown the benefits of structured trees that include syntactic information, while few methods in this branch utilized the advantages of word embeddings and another powerful technique-attention weight mechanism. This paper suggests to absorb their advantages by merging these techniques in a unified structure, dubbed as attention constituency vector tree ({ACVT}). Meanwhile, this paper develops a new tree kernel, known as {ACVT} kernel, which is tailored for sentence similarity measure based on the proposed structure. The experimental results, based on 19 widely used semantic textual similarity datasets, demonstrate that our model is effective and competitive, when compared against state-of-the-art models. Additionally, the experimental results validate that many attention weight mechanisms and word embedding techniques can be seamlessly integrated into our model, demonstrating the robustness and universality of our model.},
	pages = {853--865},
	number = {4},
	journaltitle = {{IEEE}/{ACM} Transactions on Audio, Speech, and Language Processing},
	author = {Quan, Z. and Wang, Z. and Le, Y. and Yao, B. and Li, K. and Yin, J.},
	date = {2019-04},
	keywords = {attention constituency vector tree, attention weight, attention weight mechanisms, Kernel, natural language processing, Natural language processing, natural language processing applications, neural network methods, Neural networks, semantic information, semantic textual similarity datasets, Semantics, sentence embedding, Sentence similarity, sentence similarity measure, sentence similarity modeling, Speech processing, structured trees, syntactic information, syntactic structure, Syntactics, text analysis, tree kernel, trees (mathematics), Vegetation, word embedding, word embedding techniques},
	file = {IEEE Xplore Abstract Record:/Users/tiffanychien/Zotero/storage/ZGSNU4BE/8642425.html:text/html;IEEE Xplore Full Text PDF:/Users/tiffanychien/Zotero/storage/JV9ELVGR/Quan et al. - 2019 - An Efficient Framework for Sentence Similarity Mod.pdf:application/pdf}
}

@inproceedings{glockner_breaking_2018,
	location = {Melbourne, Australia},
	title = {Breaking {NLI} Systems with Sentences that Require Simple Lexical Inferences},
	url = {https://www.aclweb.org/anthology/P18-2103},
	abstract = {We create a new {NLI} test set that shows the deficiency of state-of-the-art models in inferences that require lexical and world knowledge. The new examples are simpler than the {SNLI} test set, containing sentences that differ by at most one word from sentences in the training set. Yet, the performance on the new test set is substantially worse across systems trained on {SNLI}, demonstrating that these systems are limited in their generalization ability, failing to capture many simple inferences.},
	pages = {650--655},
	booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Glockner, Max and Shwartz, Vered and Goldberg, Yoav},
	urldate = {2019-06-06},
	date = {2018-07},
	file = {Full Text PDF:/Users/tiffanychien/Zotero/storage/GCQVC7GP/Glockner et al. - 2018 - Breaking NLI Systems with Sentences that Require S.pdf:application/pdf}
}

@inproceedings{silva_exploring_2019,
	location = {Palo Alto, California {USA}},
	title = {Exploring Knowledge Graphs in an Interpretable Composite Approach for Text Entailment},
	url = {https://www.alexandria.unisg.ch/255897/},
	abstract = {Recognizing textual entailment is a key task for many seman- tic applications, such as Question Answering, Text Summa- rization, and Information Extraction, among others. Entail- ment scenarios can range from a simple syntactic variation to more complex semantic relationships between pieces of text, but most approaches try a one-size-fits-all solution that usually favors some scenario to the detriment of another. We propose a composite approach for recognizing text entailment which analyzes the entailment pair to decide whether it must be resolved syntactically or semantically. We also make the answer interpretable: whenever an entailment is solved se- mantically, we explore a knowledge base composed of structured lexical definitions to generate natural language human- like justifications, explaining the semantic relationship hold- ing between the pieces of text. Besides outperforming well- established entailment algorithms, our composite approach gives an important step towards Explainable {AI}, using world knowledge to make the semantic reasoning process explicit and understandable.},
	booktitle = {Thirty-Third {AAAI} conference on artificial intelligence},
	publisher = {{AAAI} Press},
	author = {Silva, Vivian and Freitas, Andre and Handschuh, Siegfried},
	urldate = {2019-06-06},
	date = {2019-01-27},
	langid = {english},
	file = {Full Text PDF:/Users/tiffanychien/Zotero/storage/HVZRHSRA/Silva et al. - 2019 - Exploring Knowledge Graphs in an Interpretable Com.pdf:application/pdf;Snapshot:/Users/tiffanychien/Zotero/storage/3H5TSKUK/255897.html:text/html}
}

@article{liu_nlize:_2019,
	title = {{NLIZE}: A Perturbation-Driven Visual Interrogation Tool for Analyzing and Interpreting Natural Language Inference Models},
	volume = {25},
	issn = {1077-2626},
	doi = {10.1109/TVCG.2018.2865230},
	shorttitle = {{NLIZE}},
	abstract = {With the recent advances in deep learning, neural network models have obtained state-of-the-art performances for many linguistic tasks in natural language processing. However, this rapid progress also brings enormous challenges. The opaque nature of a neural network model leads to hard-to-debug-systems and difficult-to-interpret mechanisms. Here, we introduce a visualization system that, through a tight yet flexible integration between visualization elements and the underlying model, allows a user to interrogate the model by perturbing the input, internal state, and prediction while observing changes in other parts of the pipeline. We use the natural language inference problem as an example to illustrate how a perturbation-driven paradigm can help domain experts assess the potential limitation of a model, probe its inner states, and interpret and form hypotheses about fundamental model mechanisms such as attention.},
	pages = {651--660},
	number = {1},
	journaltitle = {{IEEE} Transactions on Visualization and Computer Graphics},
	author = {Liu, S. and Li, Z. and Li, T. and Srikumar, V. and Pascucci, V. and Bremer, P.},
	date = {2019-01},
	keywords = {Analytical models, Attention Visualization, Computational modeling, data visualisation, deep learning, hard-to-debug-systems, inference mechanisms, Interpretable Machine Learning, learning (artificial intelligence), linguistic tasks, Natural Language Inference, natural language inference problem, natural language processing, Natural Language Processing, Natural languages, neural nets, neural network model, Neural networks, perturbation-driven visual interrogation tool, Predictive models, Task analysis, Visualization, visualization system},
	file = {IEEE Xplore Abstract Record:/Users/tiffanychien/Zotero/storage/TFPZ2372/8454904.html:text/html;IEEE Xplore Full Text PDF:/Users/tiffanychien/Zotero/storage/KFJGEATY/Liu et al. - 2019 - NLIZE A Perturbation-Driven Visual Interrogation .pdf:application/pdf}
}

@article{liu_visual_2018,
	title = {Visual Interrogation of Attention-Based Models for Natural Language Inference and Machine Comprehension},
	abstract = {Neural networks models have gained unprecedented popularity in natural language processing due to their state-of-the-art performance and the ﬂexible end-to-end training scheme. Despite their advantages, the lack of interpretability hinders the deployment and reﬁnement of the models. In this work, we present a ﬂexible visualization library for creating customized visual analytic environments, in which the user can investigate and interrogate the relationships among the input, the model internals (i.e., attention), and the output predictions which in turn shed light on the model decision-making processing.},
	pages = {8},
	author = {Liu, Shusen and Li, Tao and Li, Zhimin and Srikumar, Vivek and Pascucci, Valerio and Bremer, Peer-Timo},
	date = {2018},
	langid = {english},
	file = {Liu et al. - Visual Interrogation of Attention-Based Models for.pdf:/Users/tiffanychien/Zotero/storage/RR75UTJK/Liu et al. - Visual Interrogation of Attention-Based Models for.pdf:application/pdf}
}

@article{liu_inoculation_2019,
	title = {Inoculation by Fine-Tuning: A Method for Analyzing Challenge Datasets},
	url = {http://arxiv.org/abs/1904.02668},
	shorttitle = {Inoculation by Fine-Tuning},
	abstract = {Several datasets have recently been constructed to expose brittleness in models trained on existing benchmarks. While model performance on these challenge datasets is significantly lower compared to the original benchmark, it is unclear what particular weaknesses they reveal. For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model's specific training set. We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt. We apply our method to analyze the {NLI} "stress tests" (Naik et al., 2018) and the Adversarial {SQuAD} dataset (Jia and Liang, 2017). We show that after slight exposure, some of these datasets are no longer challenging, while others remain difficult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves.},
	journaltitle = {{arXiv}:1904.02668 [cs]},
	author = {Liu, Nelson F. and Schwartz, Roy and Smith, Noah A.},
	urldate = {2019-06-06},
	date = {2019-04-04},
	eprinttype = {arxiv},
	eprint = {1904.02668},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1904.02668 PDF:/Users/tiffanychien/Zotero/storage/Y8QTHP8T/Liu et al. - 2019 - Inoculation by Fine-Tuning A Method for Analyzing.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/BDEMUAVU/1904.html:text/html}
}

@article{zhao_generating_2017,
	title = {Generating Natural Adversarial Examples},
	url = {http://arxiv.org/abs/1710.11342},
	abstract = {Due to their complex nature, it is hard to characterize the ways in which machine learning models can misbehave or be exploited when deployed. Recent work on adversarial examples, i.e. inputs with minor perturbations that result in substantially different model predictions, is helpful in evaluating the robustness of these models by exposing the adversarial scenarios where they fail. However, these malicious perturbations are often unnatural, not semantically meaningful, and not applicable to complicated domains such as language. In this paper, we propose a framework to generate natural and legible adversarial examples that lie on the data manifold, by searching in semantic space of dense and continuous data representation, utilizing the recent advances in generative adversarial networks. We present generated adversaries to demonstrate the potential of the proposed approach for black-box classifiers for a wide range of applications such as image classification, textual entailment, and machine translation. We include experiments to show that the generated adversaries are natural, legible to humans, and useful in evaluating and analyzing black-box classifiers.},
	journaltitle = {{arXiv}:1710.11342 [cs]},
	author = {Zhao, Zhengli and Dua, Dheeru and Singh, Sameer},
	urldate = {2019-06-06},
	date = {2017-10-31},
	eprinttype = {arxiv},
	eprint = {1710.11342},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv\:1710.11342 PDF:/Users/tiffanychien/Zotero/storage/VKVT3RJ4/Zhao et al. - 2017 - Generating Natural Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/8ELMJFCA/1710.html:text/html}
}

@article{kim_probing_2019,
	title = {Probing What Different {NLP} Tasks Teach Machines about Function Word Comprehension},
	url = {http://arxiv.org/abs/1904.11544},
	abstract = {We introduce a set of nine challenge tasks that test for the understanding of function words. These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of specific types of function words (e.g., prepositions, wh-words). Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, {CCG} supertagging and natural language inference ({NLI})) on the learned representations. Our results show that pretraining on {CCG}---our most syntactic objective---performs the best on average across our probing tasks, suggesting that syntactic knowledge helps function word comprehension. Language modeling also shows strong performance, supporting its widespread use for pretraining state-of-the-art {NLP} models. Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives, e.g., that {NLI} helps the comprehension of negation.},
	journaltitle = {{arXiv}:1904.11544 [cs]},
	author = {Kim, Najoung and Patel, Roma and Poliak, Adam and Wang, Alex and Xia, Patrick and {McCoy}, R. Thomas and Tenney, Ian and Ross, Alexis and Linzen, Tal and Van Durme, Benjamin and Bowman, Samuel R. and Pavlick, Ellie},
	urldate = {2019-06-06},
	date = {2019-04-25},
	eprinttype = {arxiv},
	eprint = {1904.11544},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1904.11544 PDF:/Users/tiffanychien/Zotero/storage/YV32V9KP/Kim et al. - 2019 - Probing What Different NLP Tasks Teach Machines ab.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/PTBZSRXN/1904.html:text/html}
}

@article{mccoy_right_2019,
	title = {Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference},
	url = {http://arxiv.org/abs/1902.01007},
	shorttitle = {Right for the Wrong Reasons},
	abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference ({NLI}), the task of determining whether one sentence entails another. We hypothesize that statistical {NLI} models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called {HANS} (Heuristic Analysis for {NLI} Systems), which contains many examples where the heuristics fail. We find that models trained on {MNLI}, including the state-of-the-art model {BERT}, perform very poorly on {HANS}, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in {NLI} systems, and that the {HANS} dataset can motivate and measure progress in this area.},
	journaltitle = {{arXiv}:1902.01007 [cs]},
	author = {{McCoy}, R. Thomas and Pavlick, Ellie and Linzen, Tal},
	urldate = {2019-06-06},
	date = {2019-02-03},
	eprinttype = {arxiv},
	eprint = {1902.01007},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1902.01007 PDF:/Users/tiffanychien/Zotero/storage/9PA3YE85/McCoy et al. - 2019 - Right for the Wrong Reasons Diagnosing Syntactic .pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/K3MFPTPM/1902.html:text/html}
}

@article{naik_stress_2018,
	title = {Stress Test Evaluation for Natural Language Inference},
	url = {http://arxiv.org/abs/1806.00692},
	abstract = {Natural language inference ({NLI}) is the task of determining if a natural language hypothesis can be inferred from a given premise in a justifiable manner. {NLI} was proposed as a benchmark task for natural language understanding. Existing models perform well at standard datasets for {NLI}, achieving impressive results across different genres of text. However, the extent to which these models understand the semantic content of sentences is unclear. In this work, we propose an evaluation methodology consisting of automatically constructed "stress tests" that allow us to examine whether systems have the ability to make real inferential decisions. Our evaluation of six sentence-encoder models on these stress tests reveals strengths and weaknesses of these models with respect to challenging linguistic phenomena, and suggests important directions for future work in this area.},
	journaltitle = {{arXiv}:1806.00692 [cs]},
	author = {Naik, Aakanksha and Ravichander, Abhilasha and Sadeh, Norman and Rose, Carolyn and Neubig, Graham},
	urldate = {2019-06-06},
	date = {2018-06-02},
	eprinttype = {arxiv},
	eprint = {1806.00692},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1806.00692 PDF:/Users/tiffanychien/Zotero/storage/7UZSXMXW/Naik et al. - 2018 - Stress Test Evaluation for Natural Language Infere.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/EWMNB6TZ/1806.html:text/html}
}

@article{michel_evaluation_2019,
	title = {On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models},
	url = {http://arxiv.org/abs/1903.06620},
	abstract = {Adversarial examples --- perturbations to the input of a model that elicit large changes in the output --- have been shown to be an effective way of assessing the robustness of sequence-to-sequence (seq2seq) models. However, these perturbations only indicate weaknesses in the model if they do not change the input so significantly that it legitimately results in changes in the expected output. This fact has largely been ignored in the evaluations of the growing body of related literature. Using the example of untargeted attacks on machine translation ({MT}), we propose a new evaluation framework for adversarial attacks on seq2seq models that takes the semantic equivalence of the pre- and post-perturbation input into account. Using this framework, we demonstrate that existing methods may not preserve meaning in general, breaking the aforementioned assumption that source side perturbations should not result in changes in the expected output. We further use this framework to demonstrate that adding additional constraints on attacks allows for adversarial perturbations that are more meaning-preserving, but nonetheless largely change the output sequence. Finally, we show that performing untargeted adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness, without hurting test performance. A toolkit implementing our evaluation framework is released at https://github.com/pmichel31415/teapot-nlp.},
	journaltitle = {{arXiv}:1903.06620 [cs]},
	author = {Michel, Paul and Li, Xian and Neubig, Graham and Pino, Juan Miguel},
	urldate = {2019-06-06},
	date = {2019-03-15},
	eprinttype = {arxiv},
	eprint = {1903.06620},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1903.06620 PDF:/Users/tiffanychien/Zotero/storage/WKWZ6XUC/Michel et al. - 2019 - On Evaluation of Adversarial Perturbations for Seq.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/4FT964XE/1903.html:text/html}
}

@article{logeswaran_efficient_2018,
	title = {An efficient framework for learning sentence representations},
	url = {http://arxiv.org/abs/1803.02893},
	abstract = {In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and its context, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream {NLP} tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.},
	journaltitle = {{arXiv}:1803.02893 [cs]},
	author = {Logeswaran, Lajanugen and Lee, Honglak},
	urldate = {2019-06-06},
	date = {2018-03-07},
	eprinttype = {arxiv},
	eprint = {1803.02893},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1803.02893 PDF:/Users/tiffanychien/Zotero/storage/5VC9CKQ3/Logeswaran and Lee - 2018 - An efficient framework for learning sentence repre.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/C478I5ZY/1803.html:text/html}
}

@article{liu_improving_2019,
	title = {Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding},
	url = {http://arxiv.org/abs/1904.09482},
	abstract = {This paper explores the use of knowledge distillation to improve a Multi-Task Deep Neural Network ({MT}-{DNN}) (Liu et al., 2019) for learning text representations across multiple natural language understanding tasks. Although ensemble learning can improve model performance, serving an ensemble of large {DNNs} such as {MT}-{DNN} can be prohibitively expensive. Here we apply the knowledge distillation method (Hinton et al., 2015) in the multi-task learning setting. For each task, we train an ensemble of different {MT}-{DNNs} (teacher) that outperforms any single model, and then train a single {MT}-{DNN} (student) via multi-task learning to {\textbackslash}emph\{distill\} knowledge from these ensemble teachers. We show that the distilled {MT}-{DNN} significantly outperforms the original {MT}-{DNN} on 7 out of 9 {GLUE} tasks, pushing the {GLUE} benchmark (single model) to 83.7{\textbackslash}\% (1.5{\textbackslash}\% absolute improvement{\textbackslash}footnote\{ Based on the {GLUE} leaderboard at https://gluebenchmark.com/leaderboard as of April 1, 2019.\}). The code and pre-trained models will be made publicly available at https://github.com/namisan/mt-dnn.},
	journaltitle = {{arXiv}:1904.09482 [cs]},
	author = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
	urldate = {2019-06-06},
	date = {2019-04-20},
	eprinttype = {arxiv},
	eprint = {1904.09482},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1904.09482 PDF:/Users/tiffanychien/Zotero/storage/UJV89VC4/Liu et al. - 2019 - Improving Multi-Task Deep Neural Networks via Know.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/ZURDTH2C/1904.html:text/html}
}

@inproceedings{radford_improving_2018,
	title = {Improving Language Understanding by Generative Pre-Training},
	abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering ({RACE}), and 1.5\% on textual entailment ({MultiNLI}).},
	author = {Radford, Alec},
	date = {2018},
	keywords = {Benchmark (computing), Body of uterus, cell transformation, Commonsense knowledge (artificial intelligence), Commonsense reasoning, Discriminative model, Document classification, Language model, Machine learning, Natural language understanding, Question answering, Semantic similarity, Text corpus, Textual entailment, Tracer, Transformers, Unsupervised learning},
	file = {Full Text PDF:/Users/tiffanychien/Zotero/storage/N7IIXLM9/Radford - 2018 - Improving Language Understanding by Generative Pre.pdf:application/pdf}
}

@article{gong_natural_2017,
	title = {Natural Language Inference over Interaction Space},
	url = {http://arxiv.org/abs/1709.04348},
	abstract = {Natural Language Inference ({NLI}) task requires an agent to determine the logical relationship between a natural language premise and a natural language hypothesis. We introduce Interactive Inference Network ({IIN}), a novel class of neural network architectures that is able to achieve high-level understanding of the sentence pair by hierarchically extracting semantic features from interaction space. We show that an interaction tensor (attention weight) contains semantic information to solve natural language inference, and a denser interaction tensor contains richer semantic information. One instance of such architecture, Densely Interactive Inference Network ({DIIN}), demonstrates the state-of-the-art performance on large scale {NLI} copora and large-scale {NLI} alike corpus. It's noteworthy that {DIIN} achieve a greater than 20\% error reduction on the challenging Multi-Genre {NLI} ({MultiNLI}) dataset with respect to the strongest published system.},
	journaltitle = {{arXiv}:1709.04348 [cs]},
	author = {Gong, Yichen and Luo, Heng and Zhang, Jian},
	urldate = {2019-06-06},
	date = {2017-09-13},
	eprinttype = {arxiv},
	eprint = {1709.04348},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1709.04348 PDF:/Users/tiffanychien/Zotero/storage/D9ZRJPDL/Gong et al. - 2017 - Natural Language Inference over Interaction Space.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/FENMH9A2/1709.html:text/html}
}

@article{shen_generating_2018,
	title = {Generating Contradictory, Neutral, and Entailing Sentences},
	url = {http://arxiv.org/abs/1803.02710},
	abstract = {Learning distributed sentence representations remains an interesting problem in the field of Natural Language Processing ({NLP}). We want to learn a model that approximates the conditional latent space over the representations of a logical antecedent of the given statement. In our paper, we propose an approach to generating sentences, conditioned on an input sentence and a logical inference label. We do this by modeling the different possibilities for the output sentence as a distribution over the latent representation, which we train using an adversarial objective. We evaluate the model using two state-of-the-art models for the Recognizing Textual Entailment ({RTE}) task, and measure the {BLEU} scores against the actual sentences as a probe for the diversity of sentences produced by our model. The experiment results show that, given our framework, we have clear ways to improve the quality and diversity of generated sentences.},
	journaltitle = {{arXiv}:1803.02710 [cs]},
	author = {Shen, Yikang and Tan, Shawn and Huang, Chin-Wei and Courville, Aaron},
	urldate = {2019-06-06},
	date = {2018-03-07},
	eprinttype = {arxiv},
	eprint = {1803.02710},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv\:1803.02710 PDF:/Users/tiffanychien/Zotero/storage/PBHSAWXE/Shen et al. - 2018 - Generating Contradictory, Neutral, and Entailing S.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/S9D36GZH/1803.html:text/html}
}

@article{liu_stochastic_2018,
	title = {Stochastic Answer Networks for Natural Language Inference},
	url = {http://arxiv.org/abs/1804.07888},
	abstract = {We propose a stochastic answer network ({SAN}) to explore multi-step inference strategies in Natural Language Inference. Rather than directly predicting the results given the inputs, the model maintains a state and iteratively refines its predictions. Our experiments show that {SAN} achieves the state-of-the-art results on three benchmarks: Stanford Natural Language Inference ({SNLI}) dataset, {MultiGenre} Natural Language Inference ({MultiNLI}) dataset and Quora Question Pairs dataset.},
	journaltitle = {{arXiv}:1804.07888 [cs]},
	author = {Liu, Xiaodong and Duh, Kevin and Gao, Jianfeng},
	urldate = {2019-06-06},
	date = {2018-04-21},
	eprinttype = {arxiv},
	eprint = {1804.07888},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1804.07888 PDF:/Users/tiffanychien/Zotero/storage/HZ27XGZ8/Liu et al. - 2018 - Stochastic Answer Networks for Natural Language In.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/CYA4KAG8/1804.html:text/html}
}

@article{vaswani_attention_2017,
	title = {Attention Is All You Need},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	journaltitle = {{arXiv}:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	urldate = {2019-06-07},
	date = {2017-06-12},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1706.03762 PDF:/Users/tiffanychien/Zotero/storage/HI9852FK/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/PEV9CFC3/1706.html:text/html}
}

@article{mccann_learned_2017,
	title = {Learned in Translation: Contextualized Word Vectors},
	url = {http://arxiv.org/abs/1708.00107},
	shorttitle = {Learned in Translation},
	abstract = {Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like {ImageNet}. Natural language processing ({NLP}) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep {LSTM} encoder from an attentional sequence-to-sequence model trained for machine translation ({MT}) to contextualize word vectors. We show that adding these context vectors ({CoVe}) improves performance over using only unsupervised word and character vectors on a wide variety of common {NLP} tasks: sentiment analysis ({SST}, {IMDb}), question classification ({TREC}), entailment ({SNLI}), and question answering ({SQuAD}). For fine-grained sentiment analysis and entailment, {CoVe} improves performance of our baseline models to the state of the art.},
	journaltitle = {{arXiv}:1708.00107 [cs]},
	author = {{McCann}, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
	urldate = {2019-06-07},
	date = {2017-07-31},
	eprinttype = {arxiv},
	eprint = {1708.00107},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv\:1708.00107 PDF:/Users/tiffanychien/Zotero/storage/XPJWQKRT/McCann et al. - 2017 - Learned in Translation Contextualized Word Vector.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/GQRNIT3F/1708.html:text/html}
}

@article{devlin_bert:_2018,
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	url = {http://arxiv.org/abs/1810.04805},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5\% (7.7\% point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	journaltitle = {{arXiv}:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	urldate = {2019-06-07},
	date = {2018-10-10},
	eprinttype = {arxiv},
	eprint = {1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1810.04805 PDF:/Users/tiffanychien/Zotero/storage/UZ3YKYUJ/Devlin et al. - 2018 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/tiffanychien/Zotero/storage/RZVRBUB2/1810.html:text/html}
}